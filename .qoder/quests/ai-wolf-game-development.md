# æ™ºç‹¼ (MindWolf) AIç‹¼äººæ€è½¯ä»¶æŠ€æœ¯è®¾è®¡æ–‡æ¡£

## 1. é¡¹ç›®æ¦‚è¿°

æ™ºç‹¼ (MindWolf) æ˜¯ä¸€æ¬¾åŸºäºTauri+Vue3+TypeScriptæŠ€æœ¯æ ˆå¼€å‘çš„AIç‹¼äººæ€æ¡Œé¢åº”ç”¨ã€‚è¯¥è½¯ä»¶å…è®¸å•ä¸ªäººç±»ç©å®¶ä¸å¤šä¸ªAIç©å®¶è¿›è¡Œç‹¼äººæ€æ¸¸æˆï¼ŒAIèƒ½å¤Ÿè¿›è¡Œé€»è¾‘æ¨ç†ã€å‘è¨€æ¬ºéª—å’Œç­–ç•¥åšå¼ˆï¼Œæä¾›çœŸå®çš„ç‹¼äººæ€æ¸¸æˆä½“éªŒã€‚

### 1.1 æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½AIå¯¹æ‰‹**: AIèƒ½å¤Ÿæ‰®æ¼”ä»»ä½•ç‹¼äººæ€è§’è‰²ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œè¯­è¨€æ¬ºéª—èƒ½åŠ›
- **å•äººæ¸¸æˆä½“éªŒ**: ä¸€ä½äººç±»ç©å®¶å³å¯å¼€å§‹æ¸¸æˆ
- **è·¨å¹³å°æ¡Œé¢åº”ç”¨**: åŸºäºTauriæ¡†æ¶ï¼Œæ”¯æŒWindowsã€macOSã€Linux
- **å®æ—¶è¯­éŸ³äº¤äº’**: æ”¯æŒè¯­éŸ³è¾“å…¥å’ŒAIè¯­éŸ³åˆæˆ
- **å¤ç›˜åˆ†æç³»ç»Ÿ**: è¯¦ç»†çš„æ¸¸æˆå¤ç›˜å’ŒAIå†³ç­–åˆ†æ

## 2. æŠ€æœ¯æ ˆé€‰å‹

### 2.1 å‰ç«¯æŠ€æœ¯æ ˆ
- **æ¡†æ¶**: Vue 3.x (Composition API)
- **è¯­è¨€**: TypeScript 5.x
- **æ„å»ºå·¥å…·**: Vite 4.x
- **UIç»„ä»¶åº“**: Element Plus / Naive UI
- **çŠ¶æ€ç®¡ç†**: Pinia
- **è·¯ç”±**: Vue Router 4.x
- **æ ·å¼**: SCSS + CSS Modules

### 2.2 åç«¯æŠ€æœ¯æ ˆ (Tauri Core)
- **æ¡†æ¶**: Tauri 2.x (æœ€æ–°ç¨³å®šç‰ˆ)
- **è¯­è¨€**: Rust 1.75+ (æœ€æ–°ç¨³å®šç‰ˆ)
- **AIæ¨ç†**: OpenAIå…¼å®¹API + Python (æ¨ç†å¼•æ“)
- **æ•°æ®åº“**: SQLite (æœ¬åœ°å­˜å‚¨)
- **AIæ¨¡å‹**: OpenAIå…¼å®¹APIæ¥å£ (ä¸»è¦) + æœ¬åœ°LLMå¤‡é€‰

### 2.3 AIæŠ€æœ¯æ ˆ
- **è‡ªç„¶è¯­è¨€å¤„ç†**: OpenAIå…¼å®¹API + æœ¬åœ°fallback
- **é€»è¾‘æ¨ç†å¼•æ“**: åŸºäºè§„åˆ™çš„ä¸“å®¶ç³»ç»Ÿ
- **APIå®¢æˆ·ç«¯**: reqwest (Rust) + openai-python
- **è¯­éŸ³å¤„ç†**: 
  - ASR: whisper-cpp (æœ¬åœ°è¯­éŸ³è¯†åˆ«)
  - TTS: edge-tts / æœ¬åœ°TTSå¼•æ“

## 3. ç³»ç»Ÿæ¶æ„

### 3.1 æ•´ä½“æ¶æ„å›¾

```
graph TB
    subgraph "å‰ç«¯å±‚ (Vue3 + TypeScript)"
        UI[ç”¨æˆ·ç•Œé¢]
        GM[æ¸¸æˆç®¡ç†å™¨]
        SM[çŠ¶æ€ç®¡ç† Pinia]
        Voice[è¯­éŸ³æ¨¡å—]
    end
    
    subgraph "Tauri Core (Rust)"
        API[Tauri API]
        GameEngine[æ¸¸æˆå¼•æ“]
        DB[SQLiteæ•°æ®åº“]
        Process[è¿›ç¨‹ç®¡ç†]
    end
    
    subgraph "AIåç«¯ (Python)"
        AIAgent[AIä»£ç†]
        NLP[è‡ªç„¶è¯­è¨€å¤„ç†]
        Logic[é€»è¾‘æ¨ç†å¼•æ“]
        Strategy[ç­–ç•¥æ¨¡å—]
    end
    
    subgraph "å¤–éƒ¨æœåŠ¡"
        OpenAI[OpenAIå…¼å®¹API]
        LocalLLM[æœ¬åœ°LLMå¤‡é€‰]
        TTS[è¯­éŸ³åˆæˆ]
        ASR[è¯­éŸ³è¯†åˆ«]
    end
    
    UI --> GM
    GM --> SM
    UI --> Voice
    GM --> API
    API --> GameEngine
    GameEngine --> DB
    API --> Process
    Process --> AIAgent
    AIAgent --> NLP
    AIAgent --> Logic
    AIAgent --> Strategy
    NLP --> OpenAI
    NLP --> LocalLLM
    Voice --> ASR
    Voice --> TTS
```

### 3.2 æ¨¡å—åˆ†å±‚

| å±‚çº§ | æŠ€æœ¯æ ˆ | èŒè´£ |
|------|--------|------|
| è¡¨ç°å±‚ | Vue3 + TypeScript | ç”¨æˆ·äº¤äº’ã€æ¸¸æˆç•Œé¢ã€çŠ¶æ€å±•ç¤º |
| ä¸šåŠ¡å±‚ | Tauri (Rust) | æ¸¸æˆé€»è¾‘ã€è§„åˆ™å¼•æ“ã€æ•°æ®ç®¡ç† |
| AIå±‚ | Python | AIæ¨ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ç­–ç•¥å†³ç­– |
| æ•°æ®å±‚ | SQLite | æ¸¸æˆæ•°æ®ã€é…ç½®ã€å†å²è®°å½• |

## 4. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 4.1 LLMé…ç½®ä¸ç®¡ç†æ¨¡å—

#### 4.1.1 APIé…ç½®ç®¡ç†
```typescript
interface LLMConfig {
  provider: 'openai' | 'anthropic' | 'azure' | 'custom';
  apiKey: string;
  baseUrl: string;
  model: string;
  maxTokens: number;
  temperature: number;
  timeout: number;
}

interface LLMSettings {
  primaryConfig: LLMConfig;
  fallbackConfigs: LLMConfig[];
  retryAttempts: number;
  enableLocalFallback: boolean;
  requestCaching: boolean;
}
```

#### 4.1.2 APIå®¢æˆ·ç«¯ç®¡ç†
``rust
// src-tauri/src/llm_client.rs
use reqwest::Client;
use serde_json::Value;
use std::time::Duration;

#[derive(Clone)]
pub struct LLMClient {
    client: Client,
    config: LLMConfig,
}

impl LLMClient {
    pub fn new(config: LLMConfig) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout))
            .build()
            .expect("Failed to create HTTP client");
        
        Self { client, config }
    }
    
    pub async fn chat_completion(&self, messages: Vec<ChatMessage>) -> Result<String, LLMError> {
        let request_body = json!({
            "model": self.config.model,
            "messages": messages,
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature
        });
        
        let response = self.client
            .post(&format!("{}/v1/chat/completions", self.config.base_url))
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await?;
        
        let response_json: Value = response.json().await?;
        
        // æ›´å¥å£®çš„å“åº”è§£æï¼ˆTauri 2.xä¼˜åŒ–ï¼‰
        if let Some(error) = response_json.get("error") {
            return Err(LLMError::ApiError(
                error.get("message")
                    .and_then(|m| m.as_str())
                    .unwrap_or("Unknown API error")
                    .to_string()
            ));
        }
        
        let content = response_json
            .get("choices")
            .and_then(|choices| choices.get(0))
            .and_then(|choice| choice.get("message"))
            .and_then(|message| message.get("content"))
            .and_then(|content| content.as_str())
            .ok_or_else(|| LLMError::InvalidResponse(
                "å“åº”ä¸­æœªæ‰¾åˆ°å†…å®¹".to_string()
            ))?;
        
        Ok(content.to_string())
    }
}
```

#### 4.1.3 å®¹é”™æœºåˆ¶ (Tauri 2.x å¢å¼ºç‰ˆ)
``rust
use tokio::time::{sleep, Duration};
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Clone, Debug)]
pub struct RetryConfig {
    pub max_attempts: u32,
    pub base_delay_ms: u64,
    pub max_delay_ms: u64,
}

impl Default for RetryConfig {
    fn default() -> Self {
        Self {
            max_attempts: 3,
            base_delay_ms: 1000,
            max_delay_ms: 30000,
        }
    }
}

pub struct LLMManager {
    primary_client: Arc<LLMClient>,
    fallback_clients: Vec<Arc<LLMClient>>,
    local_fallback: Option<Arc<dyn LocalLLM + Send + Sync>>,
    retry_config: RetryConfig,
    circuit_breaker: Arc<RwLock<CircuitBreaker>>,
}

// ç†”æ–­å™¨æ¨¡å¼ï¼Œé˜²æ­¢è¿ç»­å¤±è´¥
struct CircuitBreaker {
    failure_count: u32,
    last_failure_time: Option<std::time::Instant>,
    state: CircuitState,
}

#[derive(Debug, PartialEq)]
enum CircuitState {
    Closed,  // æ­£å¸¸çŠ¶æ€
    Open,    // ç†”æ–­çŠ¶æ€
    HalfOpen, // åŠå¼€çŠ¶æ€
}

impl LLMManager {
    pub fn new(
        primary_config: LLMConfig,
        fallback_configs: Vec<LLMConfig>,
    ) -> Self {
        let primary_client = Arc::new(LLMClient::new(primary_config));
        let fallback_clients = fallback_configs
            .into_iter()
            .map(|config| Arc::new(LLMClient::new(config)))
            .collect();
        
        Self {
            primary_client,
            fallback_clients,
            local_fallback: None,
            retry_config: RetryConfig::default(),
            circuit_breaker: Arc::new(RwLock::new(CircuitBreaker::new())),
        }
    }
    
    pub async fn generate_with_fallback(&self, prompt: String) -> Result<String, LLMError> {
        // æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        if self.is_circuit_open().await {
            return self.try_local_fallback(&prompt).await;
        }
        
        // ä¸»è¦APIå°è¯•
        match self.try_generate_with_retry(&self.primary_client, &prompt).await {
            Ok(result) => {
                self.reset_circuit_breaker().await;
                return Ok(result);
            }
            Err(e) => {
                log::warn!("Primary LLM failed: {}", e);
                self.record_failure().await;
            }
        }
        
        // å¤‡ç”¨APIå°è¯•
        for (index, fallback_client) in self.fallback_clients.iter().enumerate() {
            match self.try_generate_with_retry(fallback_client, &prompt).await {
                Ok(result) => {
                    log::info!("Fallback {} succeeded", index);
                    return Ok(result);
                }
                Err(e) => {
                    log::warn!("Fallback {} failed: {}", index, e);
                }
            }
        }
        
        // æœ¬åœ°æ¨¡å‹å¤‡ç”¨
        self.try_local_fallback(&prompt).await
    }
    
    async fn try_generate_with_retry(
        &self, 
        client: &LLMClient, 
        prompt: &str
    ) -> Result<String, LLMError> {
        for attempt in 1..=self.retry_config.max_attempts {
            match self.generate_single(client, prompt).await {
                Ok(result) => return Ok(result),
                Err(e) if attempt < self.retry_config.max_attempts => {
                    let delay = std::cmp::min(
                        self.retry_config.base_delay_ms * 2_u64.pow(attempt - 1),
                        self.retry_config.max_delay_ms
                    );
                    
                    log::warn!(
                        "Attempt {}/{} failed: {}, retrying in {}ms...", 
                        attempt, 
                        self.retry_config.max_attempts,
                        e,
                        delay
                    );
                    
                    sleep(Duration::from_millis(delay)).await;
                }
                Err(e) => return Err(e),
            }
        }
        unreachable!()
    }
    
    async fn generate_single(
        &self,
        client: &LLMClient,
        prompt: &str
    ) -> Result<String, LLMError> {
        let messages = vec![
            ChatMessage {
                role: "user".to_string(),
                content: prompt.to_string(),
            }
        ];
        
        client.chat_completion(messages).await
    }
    
    async fn try_local_fallback(&self, prompt: &str) -> Result<String, LLMError> {
        if let Some(local_llm) = &self.local_fallback {
            log::info!("Using local fallback model");
            local_llm.generate(prompt).await
                .map_err(|e| LLMError::ApiError(format!("Local fallback failed: {}", e)))
        } else {
            Err(LLMError::ApiError("All providers failed and no local fallback available".to_string()))
        }
    }
    
    async fn is_circuit_open(&self) -> bool {
        let breaker = self.circuit_breaker.read().await;
        breaker.state == CircuitState::Open
    }
    
    async fn record_failure(&self) {
        let mut breaker = self.circuit_breaker.write().await;
        breaker.failure_count += 1;
        breaker.last_failure_time = Some(std::time::Instant::now());
        
        if breaker.failure_count >= 5 {
            breaker.state = CircuitState::Open;
            log::warn!("Circuit breaker opened due to consecutive failures");
        }
    }
    
    async fn reset_circuit_breaker(&self) {
        let mut breaker = self.circuit_breaker.write().await;
        breaker.failure_count = 0;
        breaker.state = CircuitState::Closed;
    }
}

impl CircuitBreaker {
    fn new() -> Self {
        Self {
            failure_count: 0,
            last_failure_time: None,
            state: CircuitState::Closed,
        }
    }
}

// æœ¬åœ°LLMæ¥å£
#[async_trait::async_trait]
pub trait LocalLLM {
    async fn generate(&self, prompt: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>>;
    async fn is_available(&self) -> bool;
}
```

### 4.2 æ¸¸æˆå¼•æ“æ¨¡å—

#### 4.2.1 æ¸¸æˆçŠ¶æ€ç®¡ç†
``typescript
interface GameState {
  phase: GamePhase;           // æ¸¸æˆé˜¶æ®µ
  day: number;               // å¤©æ•°
  players: Player[];         // ç©å®¶åˆ—è¡¨
  deadPlayers: Player[];     // æ­»äº¡ç©å®¶
  votes: VoteRecord[];       // æŠ•ç¥¨è®°å½•
  gameConfig: GameConfig;    // æ¸¸æˆé…ç½®
  winner: Faction | null;    // èƒœåˆ©æ–¹
}

enum GamePhase {
  PREPARATION = 'preparation',
  NIGHT = 'night',
  DAY_DISCUSSION = 'day_discussion',
  VOTING = 'voting',
  LAST_WORDS = 'last_words',
  GAME_OVER = 'game_over'
}
```

#### 4.2.2 è§’è‰²ç³»ç»Ÿ
``typescript
abstract class Role {
  abstract roleType: RoleType;
  abstract faction: Faction;
  abstract nightAction(target?: Player): ActionResult;
  abstract canUseSkill(): boolean;
}

enum RoleType {
  WEREWOLF = 'werewolf',
  VILLAGER = 'villager',
  SEER = 'seer',
  WITCH = 'witch',
  HUNTER = 'hunter',
  GUARD = 'guard'
}
```

### 4.3 AIä»£ç†ç³»ç»Ÿ

#### 4.3.1 AIä»£ç†æ¶æ„
```
graph TD
    subgraph "AI Agent"
        Memory[è®°å¿†æ¨¡å—]
        Reasoning[æ¨ç†æ¨¡å—]
        Speech[å‘è¨€ç”Ÿæˆ]
        Strategy[ç­–ç•¥å†³ç­–]
        Personality[æ€§æ ¼æ¨¡å—]
    end
    
    GameState[æ¸¸æˆçŠ¶æ€] --> Memory
    Memory --> Reasoning
    Reasoning --> Strategy
    Strategy --> Speech
    Personality --> Speech
    Speech --> Action[è¡ŒåŠ¨è¾“å‡º]
```

#### 4.3.2 æ¨ç†å¼•æ“è®¾è®¡
``python
class ReasoningEngine:
    def __init__(self):
        self.knowledge_base = KnowledgeBase()
        self.probability_model = BayesianNetwork()
        self.rule_engine = RuleEngine()
    
    def analyze_game_state(self, game_state: GameState) -> Analysis:
        """åˆ†æå½“å‰æ¸¸æˆçŠ¶æ€ï¼Œç”Ÿæˆæ¦‚ç‡æ¨æ–­"""
        pass
    
    def update_player_suspicion(self, player_id: str, evidence: Evidence):
        """æ›´æ–°å¯¹ç©å®¶çš„æ€€ç–‘åº¦"""
        pass
    
    def generate_strategy(self, role: Role, game_state: GameState) -> Strategy:
        """æ ¹æ®è§’è‰²å’Œæ¸¸æˆçŠ¶æ€ç”Ÿæˆç­–ç•¥"""
        pass
```

#### 4.3.3 è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å—
``python
class NLPModule:
    def __init__(self, llm_manager: LLMManager):
        self.llm_manager = llm_manager
        self.intent_classifier = IntentClassifier()
        self.entity_extractor = EntityExtractor()
        self.prompt_templates = self._load_templates()
    
    def understand_speech(self, text: str, context: GameContext) -> SpeechIntent:
        """ç†è§£ç©å®¶å‘è¨€"""
        prompt = self._build_understanding_prompt(text, context)
        response = await self.llm_manager.generate_with_fallback(prompt)
        return self._parse_intent_response(response)
    
    def generate_speech(self, intent: SpeechIntent, context: GameContext, personality: AIPersonality) -> str:
        """ç”ŸæˆAIå‘è¨€"""
        prompt = self._build_generation_prompt(intent, context, personality)
        response = await self.llm_manager.generate_with_fallback(prompt)
        return self._post_process_speech(response)
    
    def _build_understanding_prompt(self, text: str, context: GameContext) -> str:
        return f"""
ä½ æ˜¯ä¸€ä¸ªç‹¼äººæ€æ¸¸æˆåˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ç©å®¶å‘è¨€çš„æ„å›¾å’Œå…³é”®ä¿¡æ¯ã€‚

æ¸¸æˆèƒŒæ™¯ï¼š
- å½“å‰æ˜¯ç¬¬{context.day}å¤©{context.phase}
- å­˜æ´»ç©å®¶ï¼š{len(context.alive_players)}äºº
- å·²çŸ¥ä¿¡æ¯ï¼š{context.public_info}

ç©å®¶å‘è¨€ï¼š"{text}"

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆJSONæ ¼å¼è¿”å›ï¼‰ï¼š
{{
  "intent": "ç«™è¾¹/æ€€ç–‘/è¾©è§£/ä¿¡æ¯åˆ†äº«/å…¶ä»–",
  "targets": ["è¢«æåŠçš„ç©å®¶ID"],
  "key_claims": ["å…³é”®å£°æ˜"],
  "emotion": "å†·é™/ç´§å¼ /æ„¤æ€’/å…¶ä»–",
  "credibility": 0.0-1.0
}}
        """
    
    def _build_generation_prompt(self, intent: SpeechIntent, context: GameContext, personality: AIPersonality) -> str:
        return f"""
ä½ æ˜¯ä¸€ä¸ªç‹¼äººæ€AIç©å®¶ï¼Œéœ€è¦æ ¹æ®å½“å‰æƒ…å†µç”Ÿæˆå‘è¨€ã€‚

è§’è‰²ä¿¡æ¯ï¼š
- ä½ çš„èº«ä»½ï¼š{context.my_role}
- ä½ çš„é˜µè¥ï¼š{context.my_faction}
- æ€§æ ¼ç‰¹ç‚¹ï¼š{personality.description}

å½“å‰æƒ…å†µï¼š
- ç¬¬{context.day}å¤©{context.phase}
- å­˜æ´»ç©å®¶ï¼š{context.alive_players}
- åœºä¸Šä¿¡æ¯ï¼š{context.situation_summary}

å‘è¨€ç›®æ ‡ï¼š{intent.objective}
å‘è¨€ç­–ç•¥ï¼š{intent.strategy}

è¦æ±‚ï¼š
1. å‘è¨€é•¿åº¦50-200å­—
2. ç¬¦åˆä½ çš„è§’è‰²èº«ä»½å’Œæ€§æ ¼
3. è‡ªç„¶æµç•…ï¼ŒåƒçœŸäººç©å®¶
4. ä¸è¦é€éœ²çœŸå®èº«ä»½ï¼ˆå¦‚æœæ˜¯ç‹¼äººï¼‰
5. ä½“ç°é€»è¾‘æ¨ç†è¿‡ç¨‹

è¯·ç”Ÿæˆå‘è¨€ï¼š
        """
    
    def analyze_deception(self, text: str, speaker_info: PlayerInfo) -> DeceptionScore:
        """åˆ†æå‘è¨€çš„æ¬ºéª—æ€§"""
        prompt = f"""
åˆ†æä»¥ä¸‹ç‹¼äººæ€å‘è¨€çš„å¯ä¿¡åº¦ï¼š

å‘è¨€è€…ä¿¡æ¯ï¼š{speaker_info}
å‘è¨€å†…å®¹ï¼š"{text}"

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{{
  "logic_consistency": 0.0-1.0,
  "information_reliability": 0.0-1.0,
  "emotion_authenticity": 0.0-1.0,
  "overall_credibility": 0.0-1.0,
  "suspicious_points": ["å¯ç–‘ç‚¹åˆ—è¡¨"]
}}
        """
        
        response = await self.llm_manager.generate_with_fallback(prompt)
        return self._parse_deception_response(response)
```

### 4.4 ç”¨æˆ·ç•Œé¢æ¨¡å—

#### 4.4.1 ç»„ä»¶æ¶æ„
```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Game/
â”‚   â”‚   â”œâ”€â”€ GameBoard.vue         # æ¸¸æˆä¸»ç•Œé¢
â”‚   â”‚   â”œâ”€â”€ PlayerCard.vue        # ç©å®¶å¡ç‰‡
â”‚   â”‚   â”œâ”€â”€ ChatPanel.vue         # èŠå¤©é¢æ¿
â”‚   â”‚   â”œâ”€â”€ VotingPanel.vue       # æŠ•ç¥¨é¢æ¿
â”‚   â”‚   â””â”€â”€ PhaseIndicator.vue    # é˜¶æ®µæŒ‡ç¤ºå™¨
â”‚   â”œâ”€â”€ AI/
â”‚   â”‚   â”œâ”€â”€ AIPlayerCard.vue      # AIç©å®¶å¡ç‰‡
â”‚   â”‚   â”œâ”€â”€ AIThinkingBubble.vue  # AIæ€è€ƒæ°”æ³¡
â”‚   â”‚   â””â”€â”€ AIPersonalitySet.vue  # AIæ€§æ ¼è®¾ç½®
â”‚   â””â”€â”€ Common/
â”‚       â”œâ”€â”€ VoiceRecorder.vue     # è¯­éŸ³å½•åˆ¶
â”‚       â”œâ”€â”€ AudioPlayer.vue       # éŸ³é¢‘æ’­æ”¾
â”‚       â””â”€â”€ GameTimer.vue         # æ¸¸æˆè®¡æ—¶å™¨
```

#### 4.4.2 çŠ¶æ€ç®¡ç†è®¾è®¡
``typescript
// stores/gameStore.ts
export const useGameStore = defineStore('game', () => {
  const gameState = ref<GameState>()
  const currentPlayer = ref<Player>()
  const chatHistory = ref<ChatMessage[]>([])
  const aiThoughts = ref<Map<string, AIThought>>()
  
  const startGame = async (config: GameConfig) => {
    // åˆå§‹åŒ–æ¸¸æˆ
  }
  
  const processPlayerAction = async (action: PlayerAction) => {
    // å¤„ç†ç©å®¶è¡ŒåŠ¨
  }
  
  const getAIResponse = async (playerId: string) => {
    // è·å–AIå“åº”
  }
  
  return {
    gameState,
    currentPlayer,
    chatHistory,
    aiThoughts,
    startGame,
    processPlayerAction,
    getAIResponse
  }
})
```

#### 4.4.3 LLMé…ç½®ç•Œé¢
``vue
<!-- LLMSettings.vue -->
<template>
  <div class="llm-settings">
    <h3>æ™ºèƒ½AIé…ç½®</h3>
    
    <!-- ä¸»è¦APIé…ç½® -->
    <div class="config-section">
      <h4>ä¸»è¦AIæœåŠ¡</h4>
      <el-form :model="primaryConfig" label-width="120px">
        <el-form-item label="æœåŠ¡æä¾›å•†">
          <el-select v-model="primaryConfig.provider">
            <el-option label="OpenAI" value="openai" />
            <el-option label="Anthropic Claude" value="anthropic" />
            <el-option label="Azure OpenAI" value="azure" />
            <el-option label="è‡ªå®šä¹‰API" value="custom" />
          </el-select>
        </el-form-item>
        
        <el-form-item label="APIåœ°å€">
          <el-input 
            v-model="primaryConfig.baseUrl" 
            placeholder="https://api.openai.com"
          />
        </el-form-item>
        
        <el-form-item label="APIå¯†é’¥">
          <el-input 
            v-model="primaryConfig.apiKey" 
            type="password" 
            show-password
            placeholder="sk-..."
          />
        </el-form-item>
        
        <el-form-item label="æ¨¡å‹åç§°">
          <el-input 
            v-model="primaryConfig.model" 
            placeholder="gpt-4"
          />
        </el-form-item>
        
        <el-form-item label="æœ€å¤§ä»¤ç‰Œ">
          <el-slider 
            v-model="primaryConfig.maxTokens" 
            :min="100" 
            :max="4000" 
            show-input
          />
        </el-form-item>
        
        <el-form-item label="åˆ›æ„åº¦">
          <el-slider 
            v-model="primaryConfig.temperature" 
            :min="0" 
            :max="2" 
            :step="0.1" 
            show-input
          />
        </el-form-item>
      </el-form>
    </div>
    
    <!-- å¤‡ç”¨é…ç½® -->
    <div class="config-section">
      <h4>å¤‡ç”¨æœåŠ¡</h4>
      <el-switch 
        v-model="enableFallback"
        active-text="å¯ç”¨å¤‡ç”¨"
        inactive-text="ç¦ç”¨å¤‡ç”¨"
      />
      <!-- å¤‡ç”¨é…ç½®è¡¨å•... -->
    </div>
    
    <!-- æµ‹è¯•åŒºåŸŸ -->
    <div class="test-section">
      <h4>è¿æ¥æµ‹è¯•</h4>
      <el-button @click="testConnection" :loading="testing">
        æµ‹è¯•è¿æ¥
      </el-button>
      <div v-if="testResult" class="test-result">
        <el-alert 
          :type="testResult.success ? 'success' : 'error'"
          :title="testResult.message"
          show-icon
        />
      </div>
    </div>
    
    <div class="action-buttons">
      <el-button type="primary" @click="saveSettings">ä¿å­˜é…ç½®</el-button>
      <el-button @click="resetToDefaults">æ¢å¤é»˜è®¤</el-button>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, reactive } from 'vue';
import { invoke } from '@tauri-apps/api/tauri';
import { useSettingsStore } from '@/stores/settingsStore';

const settingsStore = useSettingsStore();

const primaryConfig = reactive<LLMConfig>({
  provider: 'openai',
  baseUrl: 'https://api.openai.com',
  apiKey: '',
  model: 'gpt-4',
  maxTokens: 2000,
  temperature: 0.7,
  timeout: 30
});

const enableFallback = ref(false);
const testing = ref(false);
const testResult = ref<{success: boolean, message: string} | null>(null);

const testConnection = async () => {
  testing.value = true;
  testResult.value = null;
  
  try {
    const result = await invoke('test_llm_connection', {
      config: primaryConfig
    });
    
    testResult.value = {
      success: true,
      message: 'è¿æ¥æˆåŠŸï¼AIæœåŠ¡å“åº”æ­£å¸¸'
    };
  } catch (error) {
    testResult.value = {
      success: false,
      message: `è¿æ¥å¤±è´¥ï¼š${error}`
    };
  } finally {
    testing.value = false;
  }
};

const saveSettings = async () => {
  await settingsStore.updateLLMConfig(primaryConfig);
  // æ˜¾ç¤ºä¿å­˜æˆåŠŸæç¤º
};
</script>
```

### 4.5 è®¾ç½®å­˜å‚¨æ¨¡å—
```typescript
// stores/settingsStore.ts
export const useSettingsStore = defineStore('settings', () => {
  const llmConfig = ref<LLMSettings>({
    primaryConfig: {
      provider: 'openai',
      baseUrl: 'https://api.openai.com',
      apiKey: '',
      model: 'gpt-4',
      maxTokens: 2000,
      temperature: 0.7,
      timeout: 30
    },
    fallbackConfigs: [],
    retryAttempts: 3,
    enableLocalFallback: false,
    requestCaching: true
  });
  
  const loadSettings = async () => {
    try {
      const saved = await invoke('load_settings');
      if (saved.llmConfig) {
        llmConfig.value = saved.llmConfig;
      }
    } catch (error) {
      console.warn('åŠ è½½è®¾ç½®å¤±è´¥:', error);
    }
  };
  
  const saveSettings = async () => {
    try {
      await invoke('save_settings', {
        settings: {
          llmConfig: llmConfig.value
        }
      });
    } catch (error) {
      console.error('ä¿å­˜è®¾ç½®å¤±è´¥:', error);
      throw error;
    }
  };
  
  const updateLLMConfig = async (config: LLMConfig) => {
    llmConfig.value.primaryConfig = config;
    await saveSettings();
  };
  
  return {
    llmConfig,
    loadSettings,
    saveSettings,
    updateLLMConfig
  };
});
```

### 4.6 è¯­éŸ³äº¤äº’æ¨¡å—

#### 4.6.1 è¯­éŸ³è¾“å…¥å¤„ç†
```typescript
class VoiceInputHandler {
  private recorder: MediaRecorder | null = null;
  private audioChunks: Blob[] = [];
  
  async startRecording(): Promise<void> {
    // å¼€å§‹å½•éŸ³
  }
  
  async stopRecording(): Promise<string> {
    // åœæ­¢å½•éŸ³å¹¶è½¬æ¢ä¸ºæ–‡æœ¬
  }
  
  private async transcribeAudio(audioBlob: Blob): Promise<string> {
    // è°ƒç”¨Tauriåç«¯è¿›è¡Œè¯­éŸ³è¯†åˆ«
    return await invoke('transcribe_audio', { audioData: audioBlob });
  }
}
```

#### 4.6.2 è¯­éŸ³åˆæˆ
``rust
// src-tauri/src/tts.rs
use std::process::Command;

pub async fn synthesize_speech(text: String, voice_id: String) -> Result<Vec<u8>, String> {
    // è°ƒç”¨TTSå¼•æ“ç”Ÿæˆè¯­éŸ³
    let output = Command::new("python")
        .arg("ai_backend/tts_service.py")
        .arg("--text")
        .arg(&text)
        .arg("--voice")
        .arg(&voice_id)
        .output()
        .map_err(|e| e.to_string())?;
    
    Ok(output.stdout)
}
```

## 5. æ•°æ®æ¨¡å‹è®¾è®¡

### 5.1 æ•°æ®åº“è¡¨ç»“æ„

```
-- æ¸¸æˆè®°å½•è¡¨
CREATE TABLE games (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    game_id TEXT UNIQUE NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    winner_faction TEXT,
    game_config TEXT, -- JSONæ ¼å¼çš„æ¸¸æˆé…ç½®
    final_state TEXT   -- JSONæ ¼å¼çš„æœ€ç»ˆæ¸¸æˆçŠ¶æ€
);

-- ç©å®¶è¡¨
CREATE TABLE players (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    game_id TEXT NOT NULL,
    player_id TEXT NOT NULL,
    player_name TEXT NOT NULL,
    is_ai BOOLEAN NOT NULL,
    role_type TEXT NOT NULL,
    faction TEXT NOT NULL,
    is_alive BOOLEAN DEFAULT TRUE,
    death_day INTEGER,
    FOREIGN KEY (game_id) REFERENCES games(game_id)
);

-- æ¸¸æˆæ—¥å¿—è¡¨
CREATE TABLE game_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    game_id TEXT NOT NULL,
    day INTEGER NOT NULL,
    phase TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    event_type TEXT NOT NULL, -- 'speech', 'vote', 'skill', 'death'
    player_id TEXT,
    content TEXT, -- JSONæ ¼å¼çš„äº‹ä»¶å†…å®¹
    FOREIGN KEY (game_id) REFERENCES games(game_id)
);

-- è®¾ç½®é…ç½®è¡¨
CREATE TABLE settings (
    id INTEGER PRIMARY KEY,
    key TEXT UNIQUE NOT NULL,
    value TEXT NOT NULL, -- JSONæ ¼å¼çš„é…ç½®å€¼
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- LLMé…ç½®è¡¨
CREATE TABLE llm_configs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL, -- é…ç½®åç§°
    provider TEXT NOT NULL, -- 'openai', 'anthropic', 'azure', 'custom'
    base_url TEXT NOT NULL,
    api_key TEXT NOT NULL,
    model TEXT NOT NULL,
    max_tokens INTEGER DEFAULT 2000,
    temperature REAL DEFAULT 0.7,
    timeout INTEGER DEFAULT 30,
    is_primary BOOLEAN DEFAULT FALSE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### 5.2 æ ¸å¿ƒæ•°æ®ç»“æ„

``typescript
interface Player {
  id: string;
  name: string;
  isAI: boolean;
  role: Role;
  faction: Faction;
  isAlive: boolean;
  position: number; // åº§ä½å·
  aiPersonality?: AIPersonality; // AIæ€§æ ¼è®¾ç½®
}

interface ChatMessage {
  id: string;
  playerId: string;
  playerName: string;
  content: string;
  timestamp: Date;
  type: 'speech' | 'system' | 'action';
  isAI: boolean;
}

interface AIThought {
  playerId: string;
  day: number;
  phase: GamePhase;
  thoughtType: 'suspicion' | 'strategy' | 'deduction';
  content: string;
  confidence: number;
  targetPlayers?: string[]; // ç›¸å…³çš„å…¶ä»–ç©å®¶
}

interface VoteRecord {
  voterId: string;
  targetId: string;
  day: number;
  phase: GamePhase;
  timestamp: Date;
}
```

## 6. AIç­–ç•¥ä¸ç®—æ³•è®¾è®¡

### 6.1 æ¦‚ç‡æ¨ç†æ¨¡å‹

AIä½¿ç”¨è´å¶æ–¯ç½‘ç»œç»´æŠ¤å¯¹æ¯ä¸ªç©å®¶çš„æ€€ç–‘åº¦ï¼š

```
class SuspicionModel:
    def __init__(self):
        self.player_probabilities = {}  # {player_id: wolf_probability}
        self.evidence_weights = {
            'speech_contradiction': 0.3,
            'voting_pattern': 0.25,
            'night_action_result': 0.4,
            'role_claim_conflict': 0.5
        }
    
    def update_probability(self, player_id: str, evidence_type: str, evidence_strength: float):
        """æ ¹æ®è¯æ®æ›´æ–°ç©å®¶æ˜¯ç‹¼äººçš„æ¦‚ç‡"""
        current_prob = self.player_probabilities.get(player_id, 0.5)
        weight = self.evidence_weights[evidence_type]
        
        # è´å¶æ–¯æ›´æ–°
        new_prob = self._bayesian_update(current_prob, evidence_strength, weight)
        self.player_probabilities[player_id] = new_prob
```

### 6.2 ç­–ç•¥å†³ç­–æ ‘

```
graph TD
    A[AIå›åˆå¼€å§‹] --> B{æˆ‘çš„è§’è‰²ï¼Ÿ}
    B -->|ç‹¼äºº| C[ç‹¼äººç­–ç•¥]
    B -->|æ‘æ°‘| D[æ‘æ°‘ç­–ç•¥]
    B -->|ç¥èŒ| E[ç¥èŒç­–ç•¥]
    
    C --> C1{æ˜¯å¦å·²æš´éœ²ï¼Ÿ}
    C1 -->|æ˜¯| C2[æ··æ·†è§†å¬/å¸¦èŠ‚å¥]
    C1 -->|å¦| C3[ä¼ªè£…èº«ä»½/æ½œä¼]
    
    D --> D1{æ˜¯å¦æœ‰å¯é ä¿¡æ¯ï¼Ÿ}
    D1 -->|æ˜¯| D2[é€»è¾‘æ¨ç†/ç«™è¾¹]
    D1 -->|å¦| D3[è§‚å¯Ÿåˆ†æ/è·Ÿç¥¨]
    
    E --> E1{æŠ€èƒ½æ˜¯å¦å·²ç”¨ï¼Ÿ}
    E1 -->|æ˜¯| E2[å¼•å¯¼æ¨ç†/ä¿æŠ¤èº«ä»½]
    E1 -->|å¦| E3[æŠ€èƒ½ä½¿ç”¨å†³ç­–]
```

### 6.3 å‘è¨€ç”Ÿæˆç­–ç•¥

```
class SpeechGenerator:
    def __init__(self, llm_client, personality):
        self.llm_client = llm_client
        self.personality = personality
        self.speech_templates = self._load_templates()
    
    def generate_speech(self, context: GameContext, intent: SpeechIntent) -> str:
        """ç”ŸæˆAIå‘è¨€"""
        # 1. åˆ†æå½“å‰å±€åŠ¿
        situation_analysis = self._analyze_situation(context)
        
        # 2. ç¡®å®šå‘è¨€ç­–ç•¥
        strategy = self._determine_strategy(intent, situation_analysis)
        
        # 3. æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(context, strategy, self.personality)
        
        # 4. è°ƒç”¨LLMç”Ÿæˆå‘è¨€
        raw_speech = self.llm_client.generate(prompt)
        
        # 5. åå¤„ç†å’Œè¿‡æ»¤
        final_speech = self._post_process(raw_speech, context)
        
        return final_speech
    
    def _build_prompt(self, context: GameContext, strategy: str, personality: AIPersonality) -> str:
        return f"""
        ä½ æ˜¯ä¸€ä¸ªç‹¼äººæ€AIç©å®¶ï¼Œæ€§æ ¼ç‰¹ç‚¹ï¼š{personality.description}
        å½“å‰æ¸¸æˆçŠ¶å†µï¼š{context.summary}
        ä½ çš„è§’è‰²ï¼š{context.my_role}
        å‘è¨€ç­–ç•¥ï¼š{strategy}
        
        è¯·ç”Ÿæˆä¸€æ®µç¬¦åˆä½ æ€§æ ¼å’Œç­–ç•¥çš„å‘è¨€ï¼Œè¦æ±‚ï¼š
        1. å‘è¨€é•¿åº¦50-150å­—
        2. ç¬¦åˆç‹¼äººæ€æ¸¸æˆè¯­å¢ƒ
        3. ä½“ç°ä½ çš„æ€§æ ¼ç‰¹ç‚¹
        4. å®ç°å‘è¨€ç­–ç•¥ç›®æ ‡
        """
```

## 7. ç”¨æˆ·ä½“éªŒè®¾è®¡

### 7.1 æ¸¸æˆæµç¨‹è®¾è®¡

```
sequenceDiagram
    participant U as äººç±»ç©å®¶
    participant G as æ¸¸æˆå¼•æ“
    participant AI as AIç©å®¶ä»¬
    
    U->>G: å¼€å§‹æ¸¸æˆ
    G->>G: åˆ†é…è§’è‰²
    G->>U: æ˜¾ç¤ºè§’è‰²ä¿¡æ¯
    G->>AI: åˆå§‹åŒ–AIä»£ç†
    
    loop æ¸¸æˆä¸»å¾ªç¯
        G->>G: å¤œæ™šé˜¶æ®µ
        G->>AI: AIå¤œé—´è¡ŒåŠ¨
        G->>G: ç»“ç®—å¤œé—´ç»“æœ
        
        G->>G: ç™½å¤©è®¨è®º
        G->>U: è½®åˆ°ä½ å‘è¨€
        U->>G: è¯­éŸ³/æ–‡å­—å‘è¨€
        G->>AI: AIä¾æ¬¡å‘è¨€
        
        G->>G: æŠ•ç¥¨é˜¶æ®µ
        G->>U: æŠ•ç¥¨é€‰æ‹©
        U->>G: æŠ•ç¥¨ç»“æœ
        G->>AI: AIæŠ•ç¥¨
        G->>G: ç»Ÿè®¡ç¥¨æ•°
        
        alt æ¸¸æˆç»“æŸ
            G->>U: æ˜¾ç¤ºæ¸¸æˆç»“æœ
            G->>U: å¤ç›˜åˆ†æ
        end
    end
```

### 7.2 ç•Œé¢äº¤äº’è®¾è®¡

#### 7.2.1 ä¸»ç•Œé¢å¸ƒå±€
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ™ºç‹¼ MindWolf                               [è®¾ç½®] [é€€å‡º] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€ç©å®¶å¡ç‰‡åŒºâ”€â”€â”€â”€â”    â”Œâ”€æ¸¸æˆçŠ¶æ€â”€â”€â”€â”€â”    â”Œâ”€AIæ€è€ƒåŒºâ”€â”€â”€â”  â”‚
â”‚  â”‚ 1å· å¼ ä¸‰(æˆ‘)   â”‚    â”‚ ç¬¬2å¤©ç™½å¤©    â”‚    â”‚ AI-2æ­£åœ¨æ€è€ƒ â”‚  â”‚
â”‚  â”‚ ğŸ™‹â€â™‚ï¸ å­˜æ´»      â”‚    â”‚ è®¨è®ºé˜¶æ®µ     â”‚    â”‚ [æ€è€ƒæ°”æ³¡]   â”‚  â”‚
â”‚  â”‚              â”‚    â”‚ å‰©ä½™: 2:30   â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ 2å· AI-æå››   â”‚    â”‚              â”‚    â”‚ AI-3: æˆ‘è§‰å¾— â”‚  â”‚
â”‚  â”‚ ğŸ¤– å­˜æ´»      â”‚    â”‚ å­˜æ´»: 6äºº    â”‚    â”‚ 5å·å¾ˆå¯ç–‘... â”‚  â”‚
â”‚  â”‚              â”‚    â”‚ æ­»äº¡: 2äºº    â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ 3å· AI-ç‹äº”   â”‚    â”‚              â”‚    â”‚              â”‚  â”‚
â”‚  â”‚ ğŸ¤– å­˜æ´»      â”‚    â”‚              â”‚    â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€èŠå¤©è®°å½•åŒºâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ç³»ç»Ÿ: æ˜¨æ™šæ­»äº¡çš„æ˜¯7å·ç©å®¶                            â”‚  â”‚
â”‚  â”‚ 2å·: æˆ‘æ˜¯é¢„è¨€å®¶ï¼Œæ˜¨æ™šéªŒäº†3å·ï¼Œæ˜¯å¥½äºº                 â”‚  â”‚
â”‚  â”‚ 3å·: æˆ‘ä¸ä¿¡2å·ï¼Œæˆ‘è§‰å¾—ä»–æ˜¯æ‚è·³                      â”‚  â”‚
â”‚  â”‚ 4å·: è®©æˆ‘ä»¬ç†æ€§åˆ†æä¸€ä¸‹...                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€æ“ä½œåŒºâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ [ğŸ¤å¼€å§‹å½•éŸ³] [ğŸ’¬æ–‡å­—è¾“å…¥] [ğŸ—³ï¸æŠ•ç¥¨] [ğŸ“Šå¤ç›˜] [âš™ï¸è®¾ç½®]    â”‚  â”‚
â”‚  â”‚ è¾“å…¥æ¡†: [è¯·è¾“å…¥æ‚¨çš„å‘è¨€...]                [å‘é€]     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 AIä¸ªæ€§åŒ–è®¾ç½®

```typescript
interface AIPersonality {
  id: string;
  name: string;
  description: string;
  traits: {
    aggressiveness: number;     // æ”»å‡»æ€§ 0-100
    logicalness: number;        // é€»è¾‘æ€§ 0-100
    verbosity: number;          // è¯ç—¨ç¨‹åº¦ 0-100
    deceptionSkill: number;     // æ¬ºéª—æŠ€å·§ 0-100
    trustfulness: number;       // ä¿¡ä»»ä»–äººç¨‹åº¦ 0-100
  };
  speechPatterns: {
    vocabulary: 'formal' | 'casual' | 'gaming';
    lengthPreference: 'short' | 'medium' | 'long';
    emotionLevel: 'calm' | 'moderate' | 'intense';
  };
}

const predefinedPersonalities: AIPersonality[] = [
  {
    id: 'logical_analyst',
    name: 'é€»è¾‘åˆ†æå¸ˆ',
    description: 'å†·é™ç†æ€§ï¼Œå–„äºé€»è¾‘æ¨ç†ï¼Œå‘è¨€ç®€æ´æœ‰åŠ›',
    traits: { aggressiveness: 30, logicalness: 95, verbosity: 40, deceptionSkill: 60, trustfulness: 70 }
  },
  {
    id: 'aggressive_leader',
    name: 'å¼ºåŠ¿é¢†è¢–',
    description: 'ä¸»åŠ¨å¸¦èŠ‚å¥ï¼Œå‘è¨€å¼ºåŠ¿ï¼Œå–œæ¬¢æŒ‡æŒ¥ä»–äºº',
    traits: { aggressiveness: 90, logicalness: 75, verbosity: 80, deceptionSkill: 70, trustfulness: 40 }
  },
  {
    id: 'cunning_manipulator',
    name: 'ç‹¡çŒ¾æ“æ§è€…',
    description: 'å–„äºæ¬ºéª—å’Œè¯¯å¯¼ï¼Œè¯å¤šä¸”å…·æœ‰è¿·æƒ‘æ€§',
    traits: { aggressiveness: 60, logicalness: 60, verbosity: 85, deceptionSkill: 95, trustfulness: 20 }
  }
];
```

## 8. æµ‹è¯•ç­–ç•¥

### 8.1 å•å…ƒæµ‹è¯•

```
// æ¸¸æˆå¼•æ“æµ‹è¯•
describe('GameEngine', () => {
  test('should initialize game with correct roles', () => {
    const config = { playerCount: 8, roles: ['werewolf', 'seer', 'witch'] };
    const game = new GameEngine(config);
    expect(game.players).toHaveLength(8);
    expect(game.getRole('werewolf')).toBeDefined();
  });
  
  test('should handle vote correctly', () => {
    const game = createTestGame();
    game.vote('player1', 'player2');
    expect(game.getVoteCount('player2')).toBe(1);
  });
});

// AIæ¨ç†æµ‹è¯•
describe('AIReasoningEngine', () => {
  test('should update suspicion based on contradiction', () => {
    const engine = new ReasoningEngine();
    engine.updateSuspicion('player1', 'contradiction', 0.8);
    expect(engine.getSuspicion('player1')).toBeGreaterThan(0.5);
  });
});
```

### 8.2 é›†æˆæµ‹è¯•

```
describe('Game Integration', () => {
  test('should complete a full game cycle', async () => {
    const gameManager = new GameManager();
    const game = await gameManager.startGame({
      humanPlayers: 1,
      aiPlayers: 7,
      roles: 'classic'
    });
    
    // æ¨¡æ‹Ÿå®Œæ•´æ¸¸æˆæµç¨‹
    await gameManager.playNightPhase();
    await gameManager.playDayPhase();
    
    expect(game.isGameOver()).toBe(false);
  });
});
```

### 8.3 AIæ€§èƒ½æµ‹è¯•

```
class AIPerformanceTest:
    def test_ai_response_time(self):
        """æµ‹è¯•AIå“åº”æ—¶é—´"""
        ai_agent = AIAgent(personality='logical_analyst')
        start_time = time.time()
        
        response = ai_agent.generate_speech(mock_game_context)
        
        response_time = time.time() - start_time
        assert response_time < 3.0  # AIå“åº”åº”åœ¨3ç§’å†…
    
    def test_ai_consistency(self):
        """æµ‹è¯•AIè¡Œä¸ºä¸€è‡´æ€§"""
        ai_agent = AIAgent(personality='logical_analyst')
        
        # åœ¨ç›¸åŒæƒ…å†µä¸‹ï¼ŒAIåº”è¯¥åšå‡ºç±»ä¼¼çš„å†³ç­–
        decisions = []
        for _ in range(10):
            decision = ai_agent.make_decision(identical_game_state)
            decisions.append(decision)
        
        # æ£€æŸ¥å†³ç­–çš„ä¸€è‡´æ€§
        assert self.calculate_consistency(decisions) > 0.8
```

## 9. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 9.1 å‰ç«¯æ€§èƒ½ä¼˜åŒ–

```
// ä½¿ç”¨è™šæ‹Ÿæ»šåŠ¨ä¼˜åŒ–èŠå¤©è®°å½•
const ChatHistory = defineComponent({
  setup() {
    const { list, containerProps, wrapperProps } = useVirtualList(
      chatMessages,
      {
        itemHeight: 60,
        overscan: 5,
      }
    );
    
    return { list, containerProps, wrapperProps };
  }
});

// ä½¿ç”¨é˜²æŠ–ä¼˜åŒ–è¯­éŸ³è¾“å…¥
const useVoiceInput = () => {
  const processVoice = useDebounceFn(async (audioData: Blob) => {
    const text = await invoke('transcribe_audio', { audioData });
    return text;
  }, 500);
  
  return { processVoice };
};
```

### 9.2 Tauriåç«¯å®ç°

#### 9.2.1 Tauri 2.x é¡¹ç›®é…ç½®
```
# src-tauri/Cargo.toml - æœ€æ–°ç‰ˆæœ¬
[package]
name = "mindwolf"
version = "0.1.0"
edition = "2021"
rust-version = "1.75"

[build-dependencies]
tauri-build = { version = "2.0", features = ["isolation"] }

[dependencies]
tauri = { version = "2.0", features = ["isolation", "protocol-asset", "macos-private-api"] }
tauri-plugin-shell = "2.0"
tauri-plugin-fs = "2.0"
tauri-plugin-http = "2.0"
tauri-plugin-sql = { version = "2.0", features = ["sqlite"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.35", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "stream"] }
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono"] }
thiserror = "1.0"
anyhow = "1.0"
log = "0.4"
env_logger = "0.10"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.6", features = ["v4", "serde"] }
aes-gcm = "0.10"
sha2 = "0.10"
base64 = "0.21"
machine-uid = "0.3"
async-trait = "0.1"
futures = "0.3"

# æ–°å¢ï¼šTauri 2.x ä¸“ç”¨æ’ä»¶
tauri-plugin-window-state = "2.0"
tauri-plugin-notification = "2.0"
tauri-plugin-dialog = "2.0"
tauri-plugin-clipboard-manager = "2.0"

[features]
default = ["custom-protocol"]
custom-protocol = ["tauri/custom-protocol"]
```

```
// src-tauri/tauri.conf.json - Tauri 2.x é…ç½®
{
  "productName": "æ™ºç‹¼ MindWolf",
  "version": "0.1.0",
  "identifier": "com.mindwolf.app",
  "build": {
    "beforeDevCommand": "npm run dev",
    "beforeBuildCommand": "npm run build",
    "devUrl": "http://localhost:1420",
    "frontendDist": "../dist"
  },
  "app": {
    "withGlobalTauri": false,
    "windows": [
      {
        "fullscreen": false,
        "resizable": true,
        "title": "æ™ºç‹¼ MindWolf",
        "width": 1200,
        "height": 800,
        "minWidth": 800,
        "minHeight": 600,
        "center": true,
        "decorations": true,
        "transparent": false,
        "alwaysOnTop": false,
        "skipTaskbar": false,
        "theme": "auto"
      }
    ],
    "security": {
      "csp": "default-src 'self' data: blob: https://api.openai.com https://*.openai.azure.com; script-src 'self' 'unsafe-eval'; style-src 'self' 'unsafe-inline'"
    },
    "macOSPrivateApi": true
  },
  "bundle": {
    "active": true,
    "targets": "all",
    "icon": [
      "icons/32x32.png",
      "icons/128x128.png",
      "icons/128x128@2x.png",
      "icons/icon.icns",
      "icons/icon.ico"
    ],
    "category": "Game",
    "shortDescription": "AIç‹¼äººæ€æ¸¸æˆ",
    "longDescription": "æ™ºç‹¼æ˜¯ä¸€æ¬¾åŸºäºAIçš„ç‹¼äººæ€æ¸¸æˆï¼Œæ”¯æŒå•äººä¸AIå¯¹æˆ˜ï¼Œæä¾›çœŸå®çš„æ¸¸æˆä½“éªŒã€‚",
    "windows": {
      "certificateThumbprint": null,
      "digestAlgorithm": "sha256",
      "timestampUrl": ""
    },
    "macOS": {
      "entitlements": null,
      "exceptionDomain": "",
      "frameworks": [],
      "providerShortName": null,
      "signingIdentity": null
    },
    "linux": {
      "deb": {
        "depends": []
      }
    }
  },
  "plugins": {
    "shell": {
      "all": false,
      "execute": true,
      "sidecar": false,
      "open": false
    },
    "fs": {
      "all": false,
      "readFile": true,
      "writeFile": true,
      "readDir": false,
      "copyFile": false,
      "createDir": true,
      "removeDir": false,
      "removeFile": false,
      "renameFile": false,
      "exists": true
    },
    "http": {
      "all": false,
      "request": true
    },
    "sql": {
      "preload": ["sqlite:mindwolf.db"]
    }
  }
}
```

#### 9.2.2 ç°ä»£åŒ–çš„Rusté”™è¯¯å¤„ç†
```
// src-tauri/src/errors.rs - Tauri 2.x é”™è¯¯å¤„ç†
use thiserror::Error;

#[derive(Error, Debug)]
pub enum AppError {
    #[error("LLMæœåŠ¡é”™è¯¯: {0}")]
    LLMError(#[from] LLMError),
    
    #[error("æ•°æ®åº“é”™è¯¯: {0}")]
    DatabaseError(#[from] sqlx::Error),
    
    #[error("åºåˆ—åŒ–é”™è¯¯: {0}")]
    SerializationError(#[from] serde_json::Error),
    
    #[error("æ¸¸æˆçŠ¶æ€é”™è¯¯: {message}")]
    GameStateError { message: String },
    
    #[error("é…ç½®é”™è¯¯: {message}")]
    ConfigError { message: String },
    
    #[error("ç½‘ç»œé”™è¯¯: {0}")]
    NetworkError(#[from] reqwest::Error),
    
    #[error("IOé”™è¯¯: {0}")]
    IoError(#[from] std::io::Error),
}

// ä¸ºTauriå‘½ä»¤å®ç°è‡ªåŠ¨é”™è¯¯è½¬æ¢
impl From<AppError> for String {
    fn from(err: AppError) -> Self {
        err.to_string()
    }
}

// ç»Ÿä¸€çš„Resultç±»å‹
pub type AppResult<T> = Result<T, AppError>;
```

#### 9.2.3 LLMè¿æ¥æµ‹è¯•
```
// src-tauri/src/commands.rs
use crate::llm_client::LLMClient;
use tauri::State;

#[tauri::command]
pub async fn test_llm_connection(config: LLMConfig) -> Result<String, String> {
    let client = LLMClient::new(config);
    
    let test_messages = vec![
        ChatMessage {
            role: "user".to_string(),
            content: "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ä¸€ä¸‹ã€‚".to_string(),
        }
    ];
    
    match client.chat_completion(test_messages).await {
        Ok(response) => {
            if !response.trim().is_empty() {
                Ok("è¿æ¥æµ‹è¯•æˆåŠŸ".to_string())
            } else {
                Err("å“åº”ä¸ºç©º".to_string())
            }
        }
        Err(e) => Err(format!("è¿æ¥å¤±è´¥: {}", e))
    }
}

#[tauri::command]
pub async fn save_settings(settings: AppSettings) -> Result<(), String> {
    // ä¿å­˜è®¾ç½®åˆ°æ•°æ®åº“
    let db = get_database_connection().await?;
    
    let settings_json = serde_json::to_string(&settings)
        .map_err(|e| format!("åºåˆ—åŒ–å¤±è´¥: {}", e))?;
    
    sqlx::query!("INSERT OR REPLACE INTO settings (key, value) VALUES (?, ?)",
        "app_settings",
        settings_json
    )
    .execute(&db)
    .await
    .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
    
    Ok(())
}

#[tauri::command]
pub async fn load_settings() -> Result<AppSettings, String> {
    let db = get_database_connection().await?;
    
    let row = sqlx::query!("SELECT value FROM settings WHERE key = ?", "app_settings")
        .fetch_optional(&db)
        .await
        .map_err(|e| format!("æ•°æ®åº“é”™è¯¯: {}", e))?;
    
    match row {
        Some(record) => {
            serde_json::from_str(&record.value)
                .map_err(|e| format!("ååºåˆ—åŒ–å¤±è´¥: {}", e))
        }
        None => Ok(AppSettings::default())
    }
}
```

#### 9.2.4 AIä»£ç†è°ƒç”¨
```
#[tauri::command]
pub async fn generate_ai_speech(
    player_id: String,
    game_context: GameContext,
    personality: AIPersonality
) -> Result<String, String> {
    let llm_manager = get_llm_manager().await?;
    
    // æ„å»ºæç¤ºè¯
    let prompt = build_ai_speech_prompt(&game_context, &personality);
    
    // è°ƒç”¨LLMç”Ÿæˆå‘è¨€
    match llm_manager.generate_with_fallback(prompt).await {
        Ok(speech) => {
            // è®°å½•AIæ€è€ƒè¿‡ç¨‹
            log_ai_thought(&player_id, &game_context, &speech).await?;
            Ok(speech)
        }
        Err(e) => Err(format!("AIå‘è¨€ç”Ÿæˆå¤±è´¥: {}", e))
    }
}

fn build_ai_speech_prompt(context: &GameContext, personality: &AIPersonality) -> String {
    format!("""
ä½ æ˜¯ç‹¼äººæ€æ¸¸æˆAIç©å®¶ï¼Œæ€§æ ¼ç‰¹ç‚¹ï¼š{}
å½“å‰æƒ…å†µï¼š
- ä½ çš„èº«ä»½ï¼š{}
- æ¸¸æˆé˜¶æ®µï¼šç¬¬{}å¤©{}
- å­˜æ´»ç©å®¶ï¼š{}äºº
- åœºä¸Šä¿¡æ¯ï¼š{}

è¯·æ ¹æ®ä»¥ä¸Šæƒ…å†µç”Ÿæˆä¸€æ®µç¬¦åˆä½ æ€§æ ¼å’Œèº«ä»½çš„å‘è¨€ï¼š
    """,
        personality.description,
        context.my_role,
        context.day,
        context.phase,
        context.alive_players.len(),
        context.situation_summary
    )
}
```

### 9.3 AIæ¨ç†ä¼˜åŒ–

```
class OptimizedAIAgent:
    def __init__(self):
        self.reasoning_cache = {}  # ç¼“å­˜æ¨ç†ç»“æœ
        self.llm_pool = LLMPool(size=3)  # LLMè¿æ¥æ± 
    
    @lru_cache(maxsize=128)
    def analyze_game_state(self, game_state_hash: str) -> Analysis:
        """ç¼“å­˜æ¸¸æˆçŠ¶æ€åˆ†æç»“æœ"""
        pass
    
    async def generate_speech_async(self, context: GameContext) -> str:
        """å¼‚æ­¥ç”Ÿæˆå‘è¨€ï¼Œé¿å…é˜»å¡"""
        return await self.llm_pool.generate(context)
```

### 9.4 å†…å­˜ç®¡ç†

```
// Tauriåç«¯å†…å­˜ä¼˜åŒ–
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct GameStateManager {
    games: Arc<RwLock<HashMap<String, GameState>>>,
    max_games: usize,
}

impl GameStateManager {
    pub async fn cleanup_old_games(&self) {
        let mut games = self.games.write().await;
        if games.len() > self.max_games {
            // æ¸…ç†æœ€æ—§çš„æ¸¸æˆçŠ¶æ€
            let oldest_games: Vec<_> = games
                .iter()
                .sorted_by_key(|(_, state)| state.last_updated)
                .take(games.len() - self.max_games)
                .map(|(id, _)| id.clone())
                .collect();
            
            for game_id in oldest_games {
                games.remove(&game_id);
            }
        }
    }
}
```

## 10. OpenAIå…¼å®¹APIé›†æˆè¯¦è§£

### 10.1 æ”¯æŒçš„APIæä¾›å•†

æ™ºç‹¼è½¯ä»¶æ”¯æŒå¤šç§OpenAIå…¼å®¹çš„APIæœåŠ¡å•†ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

| æä¾›å•† | ä¼˜åŠ¿ | æ¨èæ¨¡å‹ | é…ç½®ç¤ºä¾‹ |
|--------|------|----------|----------|
| **OpenAI** | å®˜æ–¹æœåŠ¡ï¼Œè´¨é‡æœ€é«˜ | gpt-4, gpt-3.5-turbo | `https://api.openai.com` |
| **Azure OpenAI** | ä¼ä¸šçº§ç¨³å®šæ€§ | gpt-4, gpt-35-turbo | `https://{resource}.openai.azure.com` |
| **Anthropic Claude** | å®‰å…¨æ€§å¼ºï¼Œä¸­æ–‡å‹å¥½ | claude-3-opus, claude-3-sonnet | `https://api.anthropic.com` |
| **å›½äº§å¤§æ¨¡å‹** | æœ¬åœŸåŒ–ï¼Œå»¶è¿Ÿä½ | qwen-max, baichuan2 | å„å‚å•†APIåœ°å€ |
| **æœ¬åœ°éƒ¨ç½²** | æ•°æ®å®‰å…¨ï¼Œæˆæœ¬å¯æ§ | llama2-chinese, chatglm3 | `http://localhost:8000` |

### 10.2 APIé…ç½®è§„èŒƒ

#### 10.2.1 æ ‡å‡†OpenAIæ ¼å¼
```
{
  "provider": "openai",
  "baseUrl": "https://api.openai.com",
  "apiKey": "sk-...",
  "model": "gpt-4",
  "maxTokens": 2000,
  "temperature": 0.7,
  "timeout": 30
}
```

#### 10.2.2 Azure OpenAIæ ¼å¼
```
{
  "provider": "azure",
  "baseUrl": "https://{resource}.openai.azure.com",
  "apiKey": "your-api-key",
  "model": "gpt-4", 
  "apiVersion": "2023-12-01-preview",
  "deployment": "gpt-4-deployment-name"
}
```

#### 10.2.3 è‡ªå®šä¹‰APIæ ¼å¼
```
{
  "provider": "custom",
  "baseUrl": "https://your-api-endpoint.com",
  "apiKey": "your-api-key",
  "model": "your-model-name",
  "headers": {
    "Authorization": "Bearer {apiKey}",
    "Custom-Header": "value"
  }
}
```

### 10.3 APIè°ƒç”¨æµç¨‹

```
sequenceDiagram
    participant UI as ç”¨æˆ·ç•Œé¢
    participant Store as è®¾ç½®å­˜å‚¨
    participant Manager as LLMç®¡ç†å™¨
    participant Primary as ä¸»è¦API
    participant Fallback as å¤‡ç”¨API
    participant Local as æœ¬åœ°æ¨¡å‹
    
    UI->>Store: ä¿å­˜APIé…ç½®
    Store->>Manager: æ›´æ–°é…ç½®
    
    Note over Manager: AIéœ€è¦ç”Ÿæˆå›å¤
    Manager->>Primary: è°ƒç”¨ä¸»è¦API
    
    alt APIè°ƒç”¨æˆåŠŸ
        Primary-->>Manager: è¿”å›ç»“æœ
        Manager-->>UI: æ˜¾ç¤ºAIå›å¤
    else ä¸»è¦APIå¤±è´¥
        Manager->>Fallback: å°è¯•å¤‡ç”¨API
        alt å¤‡ç”¨APIæˆåŠŸ
            Fallback-->>Manager: è¿”å›ç»“æœ
            Manager-->>UI: æ˜¾ç¤ºAIå›å¤
        else æ‰€æœ‰äº‘ç«¯APIå¤±è´¥
            Manager->>Local: é™çº§åˆ°æœ¬åœ°æ¨¡å‹
            Local-->>Manager: æœ¬åœ°æ¨ç†ç»“æœ
            Manager-->>UI: æ˜¾ç¤ºAIå›å¤(é™çº§æ¨¡å¼)
        end
    end
```

### 10.4 æç¤ºè¯å·¥ç¨‹

#### 10.4.1 ç‹¼äººæ€ä¸“ç”¨æç¤ºè¯æ¨¡æ¿
```
const PROMPT_TEMPLATES = {
  // åŸºç¡€è§’è‰²æç¤ºè¯
  VILLAGER_BASE: `ä½ æ˜¯ç‹¼äººæ€æ¸¸æˆä¸­çš„æ‘æ°‘ï¼Œç›®æ ‡æ˜¯æ‰¾å‡ºæ‰€æœ‰ç‹¼äººã€‚ä½ éœ€è¦ï¼š
1. ä»”ç»†åˆ†ææ¯ä¸ªäººçš„å‘è¨€å’Œè¡Œä¸º
2. å¯»æ‰¾é€»è¾‘æ¼æ´å’Œå¯ç–‘ç‚¹
3. ä¸å…¶ä»–å¥½äººåä½œæ¨ç†
4. åœ¨æŠ•ç¥¨æ—¶åšå‡ºç†æ€§é€‰æ‹©`,
  
  WEREWOLF_BASE: `ä½ æ˜¯ç‹¼äººæ€æ¸¸æˆä¸­çš„ç‹¼äººï¼Œç›®æ ‡æ˜¯æ¶ˆç­æ‰€æœ‰å¥½äººè€Œä¸è¢«å‘ç°ã€‚ä½ éœ€è¦ï¼š
1. ä¼ªè£…æˆå¥½äººèº«ä»½
2. è¯¯å¯¼å…¶ä»–ç©å®¶çš„æ¨ç†æ–¹å‘
3. åœ¨é€‚å½“æ—¶æœºå¸¦èŠ‚å¥æŠ•ç¥¨
4. ä¸ç‹¼äººé˜Ÿå‹é…åˆä½†ä¸èƒ½å¤ªæ˜æ˜¾`,
  
  SEER_BASE: `ä½ æ˜¯ç‹¼äººæ€æ¸¸æˆä¸­çš„é¢„è¨€å®¶ï¼Œæ‹¥æœ‰æŸ¥éªŒèº«ä»½çš„èƒ½åŠ›ã€‚ä½ éœ€è¦ï¼š
1. åˆç†ä½¿ç”¨éªŒäººæŠ€èƒ½
2. åœ¨é€‚å½“æ—¶æœºè·³å‡ºèº«ä»½
3. ä¼ é€’å‡†ç¡®çš„éªŒäººä¿¡æ¯
4. å¼•å¯¼å¥½äººé˜µè¥çš„æ¨ç†æ–¹å‘`,
  
  // å‘è¨€åœºæ™¯æç¤ºè¯
  DEFENSE_SPEECH: `ç°åœ¨è½®åˆ°ä½ ä¸ºè‡ªå·±è¾©æŠ¤ï¼Œä½ è¢«å…¶ä»–ç©å®¶æ€€ç–‘ã€‚è¯·ï¼š
1. å†·é™åˆ†æå¯¹ä½ çš„æŒ‡æ§
2. æä¾›æœ‰åŠ›çš„åé©³è¯æ®
3. æŒ‡å‡ºçœŸæ­£çš„å¯ç–‘å¯¹è±¡
4. å±•ç°ä½ çš„é€»è¾‘æ¨ç†èƒ½åŠ›`,
  
  ACCUSATION_SPEECH: `ä½ éœ€è¦æŒ‡å‡ºä½ è®¤ä¸ºçš„ç‹¼äººå¹¶è¯´æ˜ç†ç”±ã€‚è¯·ï¼š
1. æ˜ç¡®æŒ‡å‡ºæ€€ç–‘å¯¹è±¡
2. åˆ—ä¸¾å…·ä½“çš„å¯ç–‘è¯æ®
3. åˆ†æå¯¹æ–¹çš„è¡Œä¸ºåŠ¨æœº
4. è¯´æœå…¶ä»–ç©å®¶è·Ÿä½ ç«™è¾¹`,
  
  // æ€§æ ¼åŒ–æç¤ºè¯
  AGGRESSIVE_PERSONALITY: `ä½ çš„æ€§æ ¼ç‰¹ç‚¹æ˜¯å¼ºåŠ¿ä¸”å¯Œæœ‰æ”»å‡»æ€§ï¼š
- å‘è¨€æ—¶è¯­æ°”åšå®šï¼Œæ€åº¦å¼ºç¡¬
- ä¸»åŠ¨è´¨ç–‘ä»–äººï¼Œä¸è½»æ˜“å¦¥å
- åœ¨æ¨ç†æ—¶è¡¨ç°å‡ºå¼ºçƒˆçš„è‡ªä¿¡
- å€¾å‘äºä¸»å¯¼è®¨è®ºèŠ‚å¥`,
  
  CAUTIOUS_PERSONALITY: `ä½ çš„æ€§æ ¼ç‰¹ç‚¹æ˜¯è°¨æ…ä¸”ç†æ€§ï¼š
- å‘è¨€å‰ä¼šä»”ç»†æ€è€ƒï¼Œæªè¾ä¸¥è°¨
- ä¸è½»æ˜“ä¸‹ç»“è®ºï¼Œå–œæ¬¢æ”¶é›†æ›´å¤šä¿¡æ¯
- åœ¨æ¨ç†æ—¶ä¼šè€ƒè™‘å¤šç§å¯èƒ½æ€§
- å€¾å‘äºè·Ÿéšå¤§ä¼—æ„è§è€Œéç‹¬æ–­ä¸“è¡Œ`
};
```

#### 10.4.2 åŠ¨æ€æç¤ºè¯æ„å»º
```
class PromptBuilder {
  buildSpeechPrompt(context: GameContext, intent: SpeechIntent, personality: AIPersonality): string {
    const basePrompt = this.getBaseRolePrompt(context.myRole);
    const scenarioPrompt = this.getScenarioPrompt(intent.type);
    const personalityPrompt = this.getPersonalityPrompt(personality);
    const contextPrompt = this.buildContextPrompt(context);
    
    return `${basePrompt}

${personalityPrompt}

${scenarioPrompt}

${contextPrompt}

çº¦æŸæ¡ä»¶ï¼š
1. å‘è¨€é•¿åº¦æ§åˆ¶åœ¨50-200å­—ä¹‹é—´
2. ä½¿ç”¨è‡ªç„¶çš„å£è¯­åŒ–è¡¨è¾¾
3. ä½“ç°ä½ çš„æ€§æ ¼ç‰¹ç‚¹
4. ç¬¦åˆå½“å‰æ¸¸æˆæƒ…å¢ƒ
5. ä¸è¦æš´éœ²ä½ çš„çœŸå®èº«ä»½ï¼ˆå¦‚æœæ˜¯ç‹¼äººï¼‰

è¯·ç”Ÿæˆä½ çš„å‘è¨€ï¼š`;
  }
  
  private buildContextPrompt(context: GameContext): string {
    return `å½“å‰æ¸¸æˆçŠ¶å†µï¼š
- æ¸¸æˆè¿›åº¦ï¼šç¬¬${context.day}å¤©${context.phase}
- å­˜æ´»ç©å®¶ï¼š${context.alivePlayerCount}äºº
- æ­»äº¡ç©å®¶ï¼š${context.deadPlayers.map(p => `${p.name}(${p.deathReason})`).join(', ')}
- å·²çŸ¥ä¿¡æ¯ï¼š${context.publicInfo}
- æŠ•ç¥¨æƒ…å†µï¼š${context.voteHistory}
- æœ€è¿‘å‘è¨€ï¼š${context.recentSpeech.slice(-3).map(s => `${s.speaker}: ${s.content}`).join('\n')}`;
  }
}
```

### 10.5 APIå®‰å…¨ä¸éšç§

#### 10.5.1 APIå¯†é’¥ç®¡ç†
```
// å¯†é’¥åŠ å¯†å­˜å‚¨
use aes_gcm::{Aes256Gcm, Key, Nonce};
use aes_gcm::aead::{Aead, NewAead};

pub struct SecureStorage {
    cipher: Aes256Gcm,
}

impl SecureStorage {
    pub fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let key = Self::derive_key()?;
        let cipher = Aes256Gcm::new(&key);
        Ok(Self { cipher })
    }
    
    pub fn encrypt_api_key(&self, api_key: &str) -> Result<String, Box<dyn std::error::Error>> {
        let nonce = Nonce::from_slice(b"unique nonce"); // å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨éšæœºnonce
        let ciphertext = self.cipher.encrypt(nonce, api_key.as_bytes())?;
        Ok(base64::encode(ciphertext))
    }
    
    pub fn decrypt_api_key(&self, encrypted: &str) -> Result<String, Box<dyn std::error::Error>> {
        let ciphertext = base64::decode(encrypted)?;
        let nonce = Nonce::from_slice(b"unique nonce");
        let plaintext = self.cipher.decrypt(nonce, ciphertext.as_slice())?;
        Ok(String::from_utf8(plaintext)?)
    }
    
    fn derive_key() -> Result<Key, Box<dyn std::error::Error>> {
        // ä»ç³»ç»Ÿä¿¡æ¯æ´¾ç”Ÿå¯†é’¥ï¼Œå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
        let machine_id = machine_uid::get()?;
        let mut hasher = sha2::Sha256::new();
        hasher.update(machine_id.as_bytes());
        hasher.update(b"mindwolf_secret_salt");
        let result = hasher.finalize();
        Ok(*Key::from_slice(&result))
    }
}
```

#### 10.5.2 è¯·æ±‚å†…å®¹è¿‡æ»¤
```
// æ•æ„Ÿä¿¡æ¯è¿‡æ»¤
class ContentFilter {
  private sensitivePatterns = [
    /sk-[a-zA-Z0-9]{48}/, // OpenAI APIå¯†é’¥
    /\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b/, // ä¿¡ç”¨å¡å·
    /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/, // é‚®ç®±åœ°å€
    /\b\d{11}\b/, // æ‰‹æœºå·
  ];
  
  filterGameContent(content: string): string {
    let filtered = content;
    
    // ç§»é™¤æ•æ„Ÿä¿¡æ¯
    this.sensitivePatterns.forEach(pattern => {
      filtered = filtered.replace(pattern, '[å·²è¿‡æ»¤]');
    });
    
    // é™åˆ¶å†…å®¹é•¿åº¦
    if (filtered.length > 1000) {
      filtered = filtered.substring(0, 1000) + '...';
    }
    
    return filtered;
  }
  
  validateApiKey(apiKey: string): boolean {
    // éªŒè¯APIå¯†é’¥æ ¼å¼
    const patterns = {
      openai: /^sk-[a-zA-Z0-9]{48}$/,
      anthropic: /^sk-ant-[a-zA-Z0-9\-_]{95}$/,
      custom: /^[a-zA-Z0-9\-_]{10,}$/
    };
    
    return Object.values(patterns).some(pattern => pattern.test(apiKey));
  }
}
```

### 10.6 APIæˆæœ¬æ§åˆ¶

#### 10.6.1 ä½¿ç”¨é‡ç›‘æ§
```
interface APIUsageStats {
  dailyTokens: number;
  dailyRequests: number;
  monthlyTokens: number;
  monthlyCost: number;
  lastResetDate: Date;
}

class APIUsageTracker {
  private stats: APIUsageStats;
  
  constructor() {
    this.loadStats();
  }
  
  async trackRequest(tokens: number, cost: number) {
    this.stats.dailyTokens += tokens;
    this.stats.dailyRequests += 1;
    this.stats.monthlyTokens += tokens;
    this.stats.monthlyCost += cost;
    
    await this.saveStats();
    
    // æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    this.checkLimits();
  }
  
  private checkLimits() {
    const limits = {
      dailyTokens: 100000,
      dailyRequests: 1000,
      monthlyCost: 50 // ç¾å…ƒ
    };
    
    if (this.stats.dailyTokens > limits.dailyTokens) {
      throw new Error('å·²è¾¾åˆ°æ¯æ—¥Tokenä½¿ç”¨é™åˆ¶');
    }
    
    if (this.stats.monthlyCost > limits.monthlyCost) {
      throw new Error('å·²è¾¾åˆ°æ¯æœˆè´¹ç”¨é™åˆ¶');
    }
  }
  
  getDashboardData() {
    return {
      today: {
        tokens: this.stats.dailyTokens,
        requests: this.stats.dailyRequests,
        estimatedCost: this.estimateCost(this.stats.dailyTokens)
      },
      month: {
        tokens: this.stats.monthlyTokens,
        cost: this.stats.monthlyCost
      }
    };
  }
}
```

